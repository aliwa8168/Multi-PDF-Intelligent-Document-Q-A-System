
# å¯åŠ¨ streamlit run app.py
import os
import streamlit as st


from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ============ é¡µé¢è®¾ç½® ============
st.set_page_config(page_title="Multi-PDF RAG QA", layout="wide")
st.title("å¤šPDFæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

# ============ DeepSeek API Key ============
os.environ["OPENAI_API_KEY"] = st.secrets.get("DEEPSEEK_API_KEY", "")
os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"

# ============ åˆå§‹åŒ– Session State ============
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ============ Sidebar: ä¸Šä¼  PDF ============
st.sidebar.header("ä¸Šä¼  PDF æ–‡æ¡£")
files = st.sidebar.file_uploader("ä¸Šä¼ å¤šä¸ª PDF æ–‡ä»¶", type="pdf", accept_multiple_files=True)

# ============ æ„å»ºçŸ¥è¯†åº“ ============
@st.cache_resource(show_spinner=False)
def build_vectorstore(files) -> FAISS:
    docs = []
    for file in files:
        with open(file.name, "wb") as f:
            f.write(file.getbuffer())
        loader = PyPDFLoader(file.name)
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vectorstore = FAISS.from_documents(chunks, embeddings)
    return vectorstore

if files and st.sidebar.button("æ„å»ºæ–‡æ¡£å‘é‡åº“"):
    with st.spinner("æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“..."):
        st.session_state.vectorstore = build_vectorstore(files)
    st.sidebar.success("æ–‡æ¡£åº“æ„å»ºå®Œæˆ")

# ============ RAG æ„å»º ============
def get_rag_chain(vectorstore: FAISS):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.2
    )

    prompt = ChatPromptTemplate.from_template(
        """
ä½ æ˜¯ä¸€ä¸ªåŸºäºæ–‡æ¡£çš„ä¸“ä¸šé—®ç­”åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼ä¾æ®ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹è¿›è¡Œå›ç­”ã€‚

ã€å†å²å¯¹è¯ã€‘
{history}

ã€ä¸Šä¸‹æ–‡ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

è¯·ç»™å‡ºå‡†ç¡®å›ç­”ï¼Œå¹¶åœ¨æœ€ååˆ—å‡ºå¼•ç”¨çš„æ–‡æ¡£æ¥æºä¸é¡µç ã€‚
"""
    )

    chain = (
        {
            "context": lambda x: retriever.invoke(x["question"]),
            "question": lambda x: x["question"],
            "history": lambda x: x["history"],
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# ============ èŠå¤©çª—å£ ============
st.subheader("æ–‡æ¡£é—®ç­”")

query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜")

if st.button("æé—®"):
    if st.session_state.vectorstore is None:
        st.warning("è¯·å…ˆä¸Šä¼ å¹¶æ„å»ºæ–‡æ¡£å‘é‡åº“")
    else:
        rag_chain = get_rag_chain(st.session_state.vectorstore)
        result = rag_chain.invoke({
            "question": query,
            "history": "\n".join(st.session_state.chat_history)
        })

        st.session_state.chat_history.append(f"ç”¨æˆ·ï¼š{query}")
        st.session_state.chat_history.append(f"åŠ©æ‰‹ï¼š{result}")

# ============ æ˜¾ç¤ºå†å²å¯¹è¯ ============
for msg in st.session_state.chat_history:
    if msg.startswith("ç”¨æˆ·"):
        st.markdown(f"**ğŸ§‘ {msg}**")
    else:
        st.markdown(f"ğŸ¤– {msg}")
